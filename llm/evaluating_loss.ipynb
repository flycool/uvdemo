{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the training and validation set losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a relatively small dataset for training the LLM (in fact, only one short story)\n",
    "The reasons are:\n",
    "\n",
    "You can run the code examples in a few minutes on a laptop computer without a suitable GPU.\n",
    "\n",
    "The training finishes relatively fast (minutes instead of weeks), which is good for educational purposes.\n",
    "\n",
    "We use a text from the public domain, which can be included in this GitHub repository without violating any usage rights.\n",
    "\n",
    "For example, Llama 2 7B required 184,320 GPU hours on A100 GPUs to be trained on 2 trillion tokens\n",
    "\n",
    "At the time of this writing, the hourly cost of an 8xA100 cloud server at AWS is approximately 30 dollars.\n",
    "\n",
    "So, via an off-the-envelope calculation, training this LLM would cost 184,320 / 8 * 30 = 690,000 dollars\n",
    "\n",
    "Below, we use the same dataset we used in chapter 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        text_data = response.read().decode(\"utf-8\")\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "else:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick check that the text loaded ok by printing the first and last 100 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "print(text_data[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it for me! The Strouds stand alone, and happen once--but there's no exterminating our kind of art.\"\n"
     ]
    }
   ],
   "source": [
    "print(text_data[-99:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 5,145 tokens, the text is very short for training an LLM, but again, it's for educational purposes (we will also load pretrained weights later).\n",
    "\n",
    "Next, we divide the dataset into a training and a validation set and use the data readers from chapter 2 to prepare the datasets for LLM training.\n",
    "\n",
    "Since we train the LLM to predict the next word in the text, the targets look the same as these iputs, except that the targets are shifted by one position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing the DataLoader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        for i in range(0, len(token_ids)-max_length, stride):\n",
    "            input_chunk = token_ids[i:i+max_length]\n",
    "            target_chunk = token_ids[i+1:i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    \n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,  # Vocabulary size\n",
    "    \"context_length\": 256,  # Context length\n",
    "    \"emb_dim\": 768,  # Embedding dimension\n",
    "    \"n_heads\": 12,  # Number of attention heads\n",
    "    \"n_layers\": 12,  # Number of layers\n",
    "    \"drop_rate\": 0.1,  # Dropout rate\n",
    "    \"qkv_bias\": False,  # Query-KEY-value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Warning: The training data is smaller than the context length.\")\n",
    "\n",
    "if total_tokens * (1 - train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Warning: The validation data is smaller than the context length.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a relatively small batch size to reduce the computational resource demand, and because the dataset is very small to begin with.\n",
    "\n",
    "Llama 2 7B was trained with a batch size of 1024, for example\n",
    "\n",
    "An optional check that the data was loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "9\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(len(train_loader))\n",
    "print(len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainning tokens: 4608\n",
      "Validation tokens: 512\n",
      "All tokens: 5120\n"
     ]
    }
   ],
   "source": [
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Trainning tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the GPT Model class we coded earlier. We will need this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GPTModel(nn.Module):\n",
    "#     def __init__(self, cfg):\n",
    "#         super().__init__()\n",
    "#         self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "#         self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "#         self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "#         self.trf_blocks = nn.Sequential(\n",
    "#             *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "#         )\n",
    "\n",
    "#         self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "#         self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "#     def forward(self, in_idx):\n",
    "#         batch_size, seq_len = in_idx.shape\n",
    "#         tok_embeds = self.tok_emb(in_idx)\n",
    "#         pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "#         x = tok_embeds + pos_embeds\n",
    "#         x = self.drop_emb(x)\n",
    "#         x = self.trf_blocks(x)\n",
    "#         x = self.final_norm(x)\n",
    "#         logits = self.out_head(x)   \n",
    "#         return logits\n",
    "    \n",
    "# torch.manual_seed(123)\n",
    "# model = GPTModel(GPT_CONFIG_124M)\n",
    "# model.eval() # disable dropout during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransoformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransoformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransoformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransoformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransoformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransoformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransoformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransoformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransoformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransoformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransoformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransoformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llm import GPTModel\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval() # disable dropout during inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we implement a utility function to calculate the cross-entropy loss of a given batch.\n",
    "\n",
    "In addition, we implement a second utility function to compute the loss for a user-specified number of a batches in a dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "def cal_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.0\n",
    "    if len(data_loader) == 0:\n",
    "        return float('nan')\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # reduce the number of batches to match the total number of batches\n",
    "        # if numb_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = cal_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a machine with a CUDA-supported GPU, the LLM will train on the GPU without making any changes to the code.\n",
    "\n",
    "Via the device setting, we ensure that the data is loaded onto the same device as the LLM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.987583690219456\n",
      "Validation loss: 10.982394218444824\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# disable gradient tracking for efficient because we are not  training the model yet\n",
    "with torch.no_grad():\n",
    "    train_loss = cal_loss_loader(train_loader, model, device)\n",
    "    val_loss = cal_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (step 000000): Train loss 9.798, Val loss 9.910\n",
      "Ep 1 (step 000005): Train loss 8.047, Val loss 8.324\n",
      "Ep 2 (step 000010): Train loss 6.603, Val loss 7.040\n",
      "Ep 2 (step 000015): Train loss 6.000, Val loss 6.572\n",
      "Ep 3 (step 000020): Train loss 5.513, Val loss 6.413\n",
      "Ep 3 (step 000025): Train loss 4.585, Val loss 6.358\n",
      "Ep 4 (step 000030): Train loss 3.982, Val loss 6.265\n",
      "Ep 4 (step 000035): Train loss 3.655, Val loss 6.187\n",
      "Ep 5 (step 000040): Train loss 2.913, Val loss 6.114\n",
      "Ep 6 (step 000045): Train loss 2.497, Val loss 6.173\n",
      "Ep 6 (step 000050): Train loss 1.925, Val loss 6.174\n",
      "Ep 7 (step 000055): Train loss 1.650, Val loss 6.267\n",
      "Ep 7 (step 000060): Train loss 1.069, Val loss 6.282\n",
      "Ep 8 (step 000065): Train loss 0.747, Val loss 6.390\n",
      "Ep 8 (step 000070): Train loss 0.622, Val loss 6.433\n",
      "Ep 9 (step 000075): Train loss 0.431, Val loss 6.420\n",
      "Ep 9 (step 000080): Train loss 0.329, Val loss 6.460\n",
      "Ep 10 (step 000085): Train loss 0.252, Val loss 6.579\n",
      "Training time: 7.91 minutes\n"
     ]
    }
   ],
   "source": [
    "## Training Loop for the LLM\n",
    "import torch\n",
    "import time\n",
    "\n",
    "def cal_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "def cal_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.0\n",
    "    if len(data_loader) == 0:\n",
    "        return float('nan')\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # reduce the number of batches to match the total number of batches\n",
    "        # if numb_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = cal_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches\n",
    "\n",
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() \n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # reset loss gradients from previous step\n",
    "            loss = cal_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # calculate loss gradients\n",
    "            optimizer.step() # update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel() # returns the total number of elements (or tokens ) in the input batch\n",
    "            global_step += 1\n",
    "\n",
    "            # optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "    \n",
    "        # print a sample text after each epoch\n",
    "        # generate_and_print_sample(\n",
    "        #     model, tokenizer, device, start_context\n",
    "        # )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = cal_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = cal_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "# def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "#     model.eval()\n",
    "#     context_size = model.pos_emb.weight.shape[0]\n",
    "#     encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "#     with torch.no_grad():\n",
    "#         token_ids = generate_text_simple(\n",
    "#             model=model,\n",
    "#             idx=encoded,\n",
    "#             max_new_tokens=50,\n",
    "#             context_size=context_size,\n",
    "#         )\n",
    "#     decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "#     print(decoded_text.replace('\\n', ' ')) # compat print format\n",
    "#     model.train()\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,  # Vocabulary size\n",
    "    \"context_length\": 256,  # Context length\n",
    "    \"emb_dim\": 768,  # Embedding dimension\n",
    "    \"n_heads\": 12,  # Number of attention heads\n",
    "    \"n_layers\": 12,  # Number of layers\n",
    "    \"drop_rate\": 0,  # Dropout rate\n",
    "    \"qkv_bias\": False,  # Query-KEY-value bias\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# from llm import GPTModel\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device, \n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effor moves you\",\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training time: {execution_time_minutes:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True)) # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny() # create a second x-axis sharing the same y-axis\n",
    "    ax2.plot(tokens_seen, val_losses, alpha=0) # invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout() # adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(\n",
    "    epochs_tensor,\n",
    "    tokens_seen,\n",
    "    train_losses,\n",
    "    val_losses\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
